# -*- coding: utf-8 -*-
"""PDSoepCODES.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xc4-CtYuxCWxFK7fM-F3En43tkU_5Tuw
"""

# 0.Gathering the data
import requests
import pandas as pd

# Function to get the raw URL of the CSV file from GitHub
def get_raw_csv_url(repo_url, file_path):
    base_url = 'https://raw.githubusercontent.com'
    parts = repo_url.split('/')
    user = parts[-2]
    repo_name = parts[-1]
    raw_url = f'{base_url}/{user}/{repo_name}/main/{file_path}'
    return raw_url

# GitHub repository URL and relative CSV file path
repo_url = 'https://github.com/ArpitNavadiya/project'
file_path = 'dataset.csv'  # Assuming the CSV file is directly in the repository root or in a folder named 'data'

# Get the raw URL of the CSV file
csv_url = get_raw_csv_url(repo_url, file_path)

# Fetching the CSV data
response = requests.get(csv_url)
if response.status_code == 200:
    # Read the CSV data using pandas
    df = pd.read_csv(csv_url)
    # Process the data as needed
    print(df.head())  # Display the first few rows of the CSV data
else:
    print(f"Failed to fetch CSV file from {csv_url}. Status code: {response.status_code}")

# 1. At least 5 techniques of the Data preprocessing
# 1.1 Handling missing values
import pandas as pd

# Assuming df is your DataFrame
df = pd.read_csv("PDSoepd1.csv")

# Check for missing values
missing_values = df.isnull().sum()
print("Missing values per column:")
print(missing_values)

import numpy as np
import pandas as pd

# Load the CSV file into a DataFrame
df = pd.read_csv("PDSoepd1.csv")

# Replace '?' with NaN for easier handling
df.replace('?', np.nan, inplace=True)

# Convert columns to numeric if needed
df = df.apply(pd.to_numeric, errors='ignore')

# Select numeric columns
numeric_cols = df.select_dtypes(include=np.number).columns

# Impute missing values in numeric columns with mean
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

# If there are non-numeric columns, fill missing values using forward filling (ffill)
non_numeric_cols = df.columns.difference(numeric_cols)
df[non_numeric_cols] = df[non_numeric_cols].fillna(method='ffill')

# Verify that missing values have been filled
print(df.isnull().sum())

# Save the updated DataFrame to a new CSV file
df.to_csv("updated_PDSoepd1.csv", index=False)

# 1.2 Normalization
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Assuming df is your DataFrame
df = pd.read_csv("updated_PDSoepd1.csv")  # Replace "your_dataset.csv" with your actual dataset file name

# Min-Max scaling
df_min_max_scaled = df.copy()
df_min_max_scaled[['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']] = min_max_scaler.fit_transform(df[['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']])

# Standardization
standard_scaler = StandardScaler()
df_standard_scaled = df.copy()
df_standard_scaled[['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']] = standard_scaler.fit_transform(df[['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']])

# Save the scaled DataFrames to new CSV files
df_min_max_scaled.to_csv("min_max_scaled_data.csv", index=False)
df_standard_scaled.to_csv("standard_scaled_data.csv", index=False)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Assuming df is your DataFrame
df = pd.read_csv("updated_PDSoepd1.csv")  # Replace "your_dataset.csv" with your actual dataset file name

# Min-Max scaling
min_max_scaler = MinMaxScaler()
df_min_max_scaled = df.copy()
df_min_max_scaled[['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']] = min_max_scaler.fit_transform(df[['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']])

# Standardization
standard_scaler = StandardScaler()
df_standard_scaled = df.copy()
df_standard_scaled[['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']] = standard_scaler.fit_transform(df[['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']])

# Save the scaled DataFrames to new CSV files
df_min_max_scaled.to_csv("min_max_scaled_data.csv", index=False)
df_standard_scaled.to_csv("standard_scaled_data.csv", index=False)

# 1.3 Encoding categorical variables
import pandas as pd

# Assuming df is your DataFrame with the required columns
df = pd.get_dummies(df, columns=['TimeOfDay', 'DrowsinessLevel'])

# Save the updated DataFrame to a new CSV file
df.to_csv('updated_data.csv', index=False)

# 1.4 Outlier detection and removal
import pandas as pd
import numpy as np
from scipy import stats

# Assuming df is your DataFrame with the required columns
columns_to_check = ['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']

# Calculate Z-scores
z_scores = np.abs(stats.zscore(df[columns_to_check]))

# Remove rows with outliers (Z-score > 3)
df = df[(z_scores < 3).all(axis=1)]

# Save the updated DataFrame to a new CSV file
df.to_csv('outliers_removed.csv', index=False)

# 1.5 Feature Scaling
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming df is your DataFrame with the required columns
columns_to_scale = ['EyeClosureDuration', 'BlinkRate', 'HeadMovement', 'SteeringWheelMovement']

# Initialize Min-Max scaler
scaler = MinMaxScaler()

# Perform Min-Max scaling for selected columns
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

# Save the updated DataFrame to a new CSV file
df.to_csv('scaled_data.csv', index=False)

# 2. Descriptive Statistical Analysis
import pandas as pd

# Load the CSV file into a DataFrame
df = pd.read_csv('updated_PDSoepd1.csv')

# Display descriptive statistics using describe()
description = df.describe()

# Display the descriptive statistics
print(description)

# 3.  Visual Analysis
# 3.1 Using matplotlib
import pandas as pd

# Load the data from the CSV file into a DataFrame
file_path = 'PDSoepd1.csv'
df = pd.read_csv(file_path)

# Display the first few rows of the DataFrame to check if the data is loaded correctly
print(df.head())

# Check for missing values in the DataFrame
missing_values = df.isnull().sum()
print("\nMissing values:\n", missing_values)

# Drop rows with missing values if needed
# df.dropna(inplace=True)

# Perform data analysis, visualization, or any other tasks on this DataFrame
# For example, you can plot a histogram for the 'EyeClosureDuration' column
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.hist(df['EyeClosureDuration'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
plt.title('Histogram of Eye Closure Duration')
plt.xlabel('Eye Closure Duration')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# 3.2 Using sns
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the CSV file into a pandas DataFrame
df = pd.read_csv('PDSoepd1.csv')

# Extract the column for plotting (assuming 'EyeClosureDuration' exists)
eye_closure_duration = df['EyeClosureDuration']

# Plotting histogram using Seaborn
plt.figure(figsize=(8, 6))
sns.histplot(eye_closure_duration, kde=True, color='skyblue', bins=30)
plt.title('Histogram of Eye Closure Duration')
plt.xlabel('Eye Closure Duration')
plt.ylabel('Frequency')
plt.show()

# 4. Feature Extraction based on correlation, covariance
import pandas as pd
import numpy as np

# Load the CSV file into a pandas DataFrame
df = pd.read_csv('updated_PDSoepd1.csv')

# Calculate the correlation matrix
corr_matrix = df.corr()

# Display the correlation matrix
print("Correlation Matrix:")
print(corr_matrix)

# Extract features based on correlation threshold
threshold = 0.5  # Adjust this threshold as needed
corr_features = set()
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            colname = corr_matrix.columns[i]
            corr_features.add(colname)

print("\nFeatures selected based on correlation:")
print(corr_features)

# Calculate the covariance matrix
cov_matrix = df.cov()

# Display the covariance matrix
print("\nCovariance Matrix:")
print(cov_matrix)

# Extract features based on covariance threshold
threshold_cov = 100  # Adjust this threshold as needed
cov_features = set()
for i in range(len(cov_matrix.columns)):
    for j in range(i):
        if abs(cov_matrix.iloc[i, j]) > threshold_cov:
            colname = cov_matrix.columns[i]
            cov_features.add(colname)

print("\nFeatures selected based on covariance:")
print(cov_features)

import pandas as pd

# Load the CSV file into a pandas DataFrame
df = pd.read_csv('updated_PDSoepd1.csv')

# Select only numeric columns
numeric_cols = df.select_dtypes(include=[np.number])

# Calculate the correlation matrix
corr_matrix = numeric_cols.corr()

# Display the correlation matrix
print("Correlation Matrix:")
print(corr_matrix)

# Extract features based on correlation threshold
threshold = 0.5  # Adjust this threshold as needed
corr_features = set()
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            colname = corr_matrix.columns[i]
            corr_features.add(colname)

print("\nFeatures selected based on correlation:")
print(corr_features)

# Calculate the covariance matrix
cov_matrix = numeric_cols.cov()

# Display the covariance matrix
print("\nCovariance Matrix:")
print(cov_matrix)

# Extract features based on covariance threshold
threshold_cov = 100  # Adjust this threshold as needed
cov_features = set()
for i in range(len(cov_matrix.columns)):
    for j in range(i):
        if abs(cov_matrix.iloc[i, j]) > threshold_cov:
            colname = cov_matrix.columns[i]
            cov_features.add(colname)

print("\nFeatures selected based on covariance:")
print(cov_features)

# 5. Models Creation ( At Least two)
# 5.1 Random forest classification model

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import KBinsDiscretizer

# Load the dataset from csv
df = pd.read_csv('updated_PDSoepd1.csv')

# Extract features and target variable
X = df.iloc[:, 0].values.reshape(-1, 1)  # Reshape X to a 2D array
y = df.iloc[:, 1].values

# Binning the continuous target variable into categories
kbins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')  # Adjust the number of bins as needed
y_binned = kbins.fit_transform(y.reshape(-1, 1))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_binned, test_size=0.3, random_state=42)

# Create a Random Forest classifier with 100 trees and random state set to 42
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
rf_classifier.fit(X_train, y_train.ravel())  # Use ravel() to flatten y_train

# Make predictions
y_pred = rf_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

import seaborn as sns

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(4, 4))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt="g")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn import tree
for i in range(5):
  # Select a tree from the Random Forest (e.g., the first tree)
  tree_to_visualize = rf_classifier.estimators_[i]

  # Plot the selected tree
  plt.figure(figsize=(15, 15))
  tree.plot_tree(tree_to_visualize, filled=True)
  plt.show()

# 5.2 Decision tree classification model
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the dataset from csv
df = pd.read_csv('updated_PDSoepd1.csv')

# Extract features and target variable
X = df.iloc[:, :-1]  # Features
y = df.iloc[:, -1]   # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Define categorical columns for one-hot encoding
categorical_cols = [col for col in X.columns if X[col].dtype == 'object']

# Apply one-hot encoding to categorical columns
preprocessor = ColumnTransformer(transformers=[('onehot', OneHotEncoder(), categorical_cols)], remainder='passthrough')
X_train_encoded = preprocessor.fit_transform(X_train)
X_test_encoded = preprocessor.transform(X_test)

# Create a Decision Tree classifier with default parameters
dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=0)

# Train the classifier
dt_classifier.fit(X_train_encoded, y_train)

# Make predictions
y_pred = dt_classifier.predict(X_test_encoded)

print(confusion_matrix(y_test, y_pred))

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Plot the selected tree
plt.figure(figsize=(10, 10))
plot_tree(rf_classifier.estimators_[0], filled=True)  # Plotting the first tree of the RandomForestClassifier
plt.show()

# # 6. Evaluation of the model based on various metrics
# 6.1 Random forest classification model
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Load the dataset from csv
df = pd.read_csv('updated_PDSoepd1.csv')

# Check the column names in your DataFrame
print(df.columns)

# Assuming 'y' is the correct column name for the target variable
target_column = 'y'

# Extract features and target variable
X = df.iloc[:, 0].values.reshape(-1, 1)  # Reshape X to a 2D array
y = df['DrowsinessLevel']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a Random Forest classifier with 100 trees and random state set to 42
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
rf_classifier.fit(X_train, y_train)

# Make predictions
y_pred = rf_classifier.predict(X_test)

# Calculate precision, recall, f1-score, and support
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

# 6.2 Decision tree classification model

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer

# Load the dataset from csv
df = pd.read_csv('updated_PDSoepd1.csv')

# Extract features and target variable
X = df.iloc[:, 0].values.reshape(-1, 1)  # Reshape X to a 2D array
y = df.iloc[:, 1].values

# Binning the continuous target variable into categories
kbins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')  # Adjust the number of bins as needed
y_binned = kbins.fit_transform(y.reshape(-1, 1))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_binned, test_size=0.3, random_state=42)

# Create and train the Decision Tree classifier
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = clf.predict(X_test)

# Print the confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Print the classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# Load the dataset from csv
df = pd.read_csv('updated_PDSoepd1.csv')

# Check the column names in your DataFrame
print(df.columns)

# Assuming 'y' is the correct column name for the target variable
target_column = 'y'

# Extract features and target variable
X = df.iloc[:, 0].values.reshape(-1, 1)  # Reshape X to a 2D array
y = df['DrowsinessLevel']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a Decision Tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Train the classifier
dt_classifier.fit(X_train, y_train)

# Make predictions
y_pred = dt_classifier.predict(X_test)

# Calculate precision, recall, f1-score, and support
report = classification_report(y_test, y_pred)
print("Classification Report for Decision Tree:\n", report)